"""
Scraper as√≠ncrono para extraer noticias de m√∫ltiples fuentes en paralelo.

Este m√≥dulo orquesta el proceso de scraping usando componentes modulares:
- AsyncHTTPClient: Cliente HTTP as√≠ncrono
- ArticleFinder: Encuentra art√≠culos en portadas
- NewsExtractor: Extrae datos de noticias individuales
- DateParser: Parsea fechas de publicaci√≥n
- ContentParser: Extrae contenido (t√≠tulo, cuerpo, etc.)
"""
import asyncio
import time
from bs4 import BeautifulSoup

from app.models.noticia import Noticia
from app.http import AsyncHTTPClient
from app.extractors import ArticleFinder, NewsExtractor
from config.settings import get_scraping_config
from config.logging_config import get_logger

logger = get_logger(__name__)

class AsyncNewsScraper:
    """
    Scraper as√≠ncrono de noticias.
    
    Coordina el proceso de scraping de m√∫ltiples fuentes de forma
    paralela y eficiente.
    """
    
    def __init__(self, max_concurrent_requests: int = None):
        """
        Inicializa el scraper.
        
        Args:
            max_concurrent_requests: L√≠mite de requests concurrentes.
                                   Si es None, usa el valor de configuraci√≥n.
        """
        self.config = get_scraping_config()
        self.max_concurrent = max_concurrent_requests or self.config['max_concurrent_requests']
        
        # Componentes
        self.http_client = AsyncHTTPClient()
        self.article_finder = ArticleFinder()
    
    async def _scrape_source(
        self,
        session,
        source: dict,
        semaphore: asyncio.Semaphore,
        news_extractor: NewsExtractor
    ) -> list[Noticia]:
        """
        Scrapea una fuente individual.
        
        Args:
            session: Sesi√≥n HTTP activa
            source: Dict con 'name' y 'url' del diario
            semaphore: Sem√°foro para limitar concurrencia
            news_extractor: Extractor de noticias
            
        Returns:
            Lista de noticias extra√≠das
        """
        source_url = source['url']
        source_name = source['name']
        start_time = time.time()
        
        try:
            logger.info(f"üîç Scrapeando: {source_name}")
            
            # Obtener HTML de la portada
            html = await self.http_client.fetch_html(session, source_url)
            if not html:
                logger.error(f"   ‚ùå No se pudo obtener HTML")
                return []
            
            soup = BeautifulSoup(html, "html.parser")
            
            # Encontrar art√≠culos
            articles = self.article_finder.encontrar_articulos(soup)
            if not articles:
                logger.warning(f"   ‚ö†Ô∏è No se encontraron art√≠culos")
                return []
            
            # Extraer URLs
            urls = self.article_finder.extraer_urls(articles, source_url)
            logger.info(f"   üì∞ {source_name}:{len(articles)} art√≠culos encontrados, procesando {len(urls)} URLs...")
            
            # Procesar noticias en paralelo
            tasks = [
                news_extractor.extraer(session, url, source_name, semaphore)
                for url in urls
            ]
            
            resultados = await asyncio.gather(*tasks)
            noticias = [n for n in resultados if n is not None]
            
            elapsed = time.time() - start_time
            logger.info(f"   ‚úÖ {source_name}: {len(noticias)} noticias extra√≠das en {elapsed:.2f}s")
            
            return noticias
            
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"   ‚ùå Error: {e} ({elapsed:.2f}s)")
            return []
    
    async def scrape_async(self, sources: list[dict]) -> list[Noticia]:
        """
        Scrapea todas las fuentes de forma as√≠ncrona.
        
        Args:
            sources: Lista de dicts con 'name' y 'url'
            
        Returns:
            Lista de todas las noticias extra√≠das
        """
        start_time = time.time()
        
        logger.info("=" * 60)
        logger.info("üöÄ SCRAPING AS√çNCRONO - MODO PARALELO")
        logger.info("=" * 60)
        logger.info(f"   Fuentes: {len(sources)}")
        logger.info(f"   Conexiones concurrentes: {self.max_concurrent}")
        
        semaphore = asyncio.Semaphore(self.max_concurrent)
        news_extractor = NewsExtractor(self.http_client)
        
        async with self.http_client.crear_session(self.max_concurrent) as session:
            # Procesar todas las fuentes en paralelo
            tasks = [
                self._scrape_source(session, source, semaphore, news_extractor)
                for source in sources
            ]
            
            resultados_por_fuente = await asyncio.gather(*tasks)
        
        # Combinar todas las noticias
        todas_las_noticias = []
        for noticias in resultados_por_fuente:
            todas_las_noticias.extend(noticias)
        
        elapsed = time.time() - start_time
        
        self._imprimir_resumen(len(sources), len(todas_las_noticias), elapsed)
        
        return todas_las_noticias
    
    def _imprimir_resumen(self, num_fuentes: int, num_noticias: int, elapsed: float):
        """Imprime el resumen final del scraping."""
        logger.info(f"{'=' * 60}")
        logger.info(f"üìä RESUMEN FINAL:")
        logger.info(f"   Fuentes procesadas: {num_fuentes}")
        logger.info(f"   Total noticias: {num_noticias}")
        logger.info(f"   ‚è±Ô∏è  TIEMPO TOTAL: {elapsed:.2f}s ({elapsed/60:.2f} min)")
        if num_noticias:
            logger.info(f"   ‚ö° Promedio: {elapsed/num_noticias:.2f}s/noticia")
        logger.info("=" * 60)
    
    def scrape(self, sources: list[dict]) -> list[Noticia]:
        """
        Wrapper s√≠ncrono para scrape_async.
        
        Args:
            sources: Lista de dicts con 'name' y 'url'
            
        Returns:
            Lista de todas las noticias extra√≠das
        """
        return asyncio.run(self.scrape_async(sources))


# Exportar clase principal
__all__ = ['AsyncNewsScraper']
